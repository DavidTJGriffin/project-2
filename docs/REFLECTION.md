# Reflections Log

This document serves as a log of reflections on various topics, capturing insights, lessons learned, and personal growth. It is intended to help users track their thoughts and experiences over time.

Overall, this lab was really enjoyable. I genuinely like mathematics, so it was nice to see how the model handled it.

Throughout this project I used Claude Opus 4.6 with GitHub Copilot, specifically within VS Code rather than IntelliJ. There seems to be a significant difference in the quality and interactivity of GitHub Copilot between the two IDEs. You can actually see it in the extensions marketplace: Copilot has a much higher rating in VS Code than in the IntelliJ plugins. Having an AI agent continuously working through tasks, checking itself, and maintaining to-do lists is most effective when it's integrated directly in the IDE. It closes the feedback loop and makes the whole experience more seamless.

For projects that are more complex or requirement-heavy, using AI agents inside an IDE is far more advantageous than using a chat interface. The agents can read surrounding files for additional context, and you can even include markdown files directly in the codebase to inform prompt engineering. You can prompt it to read specific files, tell it to check itself, and it can even run terminal commands to test the code, which is genuinely useful.

I really liked the output of Claude Opus 4.6, though it does tend to be a bit verbose in the code it generates. This time, because I front-loaded it with a solid context and a comprehensive prompt, it produced shorter, well-structured classes with various methods. After I reviewed and ran the tests, which all passed, I had a suspicion that some requirements from the README might be missing. Since I was using Copilot inside the IDE, I just prompted it to go back, review the full codebase against the README, and determine whether all requirements were actually satisfied. Using AI collaboratively in this way can genuinely help you catch gaps in requirements that you'd otherwise miss. That said, maintaining a human in the loop remains important.

I really enjoyed the comments it left in the code. They were well-organized, and I loved the JavaDocs formatting. One of my favorite touches was how it organized code into sections separated by dashes, which gave you a higher-level overview before diving into the details. Within the classes themselves, sections were labeled things like "object overrides," "getters and setters," "abstract methods," and "calculation implementation," making the code logical and quick to navigate. The readability was genuinely impressive. Seeing the class or method name, parameters, and return values upfront lets you immediately understand the inputs, outputs, and purpose of a function before you even read the implementation.

Overall this was a great exercise, and I feel like I've improved my prompting skills through it. One of my favorite techniques is to use AI in a meta way: draft a thorough prompt myself, have it improve that prompt, then feed the improved version back in. This consistently produces strong results because it surfaces gaps, raises clarifying questions, and sharpens the input. Better input quality means better output quality.